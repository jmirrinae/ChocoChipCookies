{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainTextSimilarity(siameseLSTM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwtkTKSGsU4V"
      },
      "source": [
        "##Siamese LSTM을 이용한 [자소서문장-면접질문] 유사도 학습\n",
        "* MaLSTM 모델 출처 : https://docs.likejazz.com/siamese-lstm/\n",
        "* pre-trained 모델 : https://github.com/Kyubyong/wordvectors\n",
        "* okt tokenizer 출처 : https://blog.breezymind.com/2018/03/02/sklearn-feature_extraction-text-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmhBWR3epYKu"
      },
      "source": [
        "###1. 필요한 패키지 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yHd_xQOpn8p"
      },
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "import gensim\n",
        "\n",
        "import itertools\n",
        "\n",
        "from time import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from tensorflow.python.keras.layers import Input, Embedding, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6nm8wRMEU6b"
      },
      "source": [
        "###2. 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuvJxSkpwPip"
      },
      "source": [
        "okt = Okt()\n",
        "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,알파벳,숫자 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
        "def text_to_word_list(raw, pos=[\"Noun\",\"Alpha\",], stopword=['로서', '면', '일지', '위해', '대해', '무엇', '어디', '또한', '대한', '통해']):  #pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"]\n",
        "    raw = re.sub(r\"e-mail\", \"email\", raw)\n",
        "    raw = re.sub(r\"e - mail\", \"email\", raw)\n",
        "    raw = re.sub(r\"\\[\", \"\", raw)\n",
        "    raw = re.sub(r\"\\]\", \"\", raw)\n",
        "    raw = re.sub(r\"\\.\", \" \", raw)\n",
        "    \n",
        "    return [\n",
        "        word for word, tag in okt.pos(\n",
        "            raw,\n",
        "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
        "            stem=True    # stemming 바뀌나->바뀌다\n",
        "            )\n",
        "            if len(word) > 1 and tag in pos and word not in stopword\n",
        "        ]\n",
        "\n",
        "\n",
        "def make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n",
        "    vocabs = {}\n",
        "    vocabs_cnt = 0\n",
        "\n",
        "    vocabs_not_w2v = {}\n",
        "    vocabs_not_w2v_cnt = 0\n",
        "\n",
        "    # Stopwords (text_to_word_list에서 stopword를 확인했으므로, 여기서는 stopwords 지정하지 않음)\n",
        "    stops = []\n",
        "\n",
        "    # Load word2vec\n",
        "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
        "\n",
        "    if empty_w2v:\n",
        "        word2vec = EmptyWord2Vec\n",
        "    else:\n",
        "        word2vec = gensim.models.word2vec.Word2Vec.load(\"./ko.bin\").wv\n",
        "        \n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Print the number of embedded sentences.\n",
        "        if index != 0 and index % 1000 == 0:\n",
        "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in ['자소서문장', '면접질문']:\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "            \n",
        "                # Check for unwanted words\n",
        "                if word in stops:\n",
        "                    continue\n",
        "\n",
        "                # If a word is missing from word2vec model.\n",
        "                if word not in word2vec.vocab:\n",
        "                    if word not in vocabs_not_w2v:\n",
        "                        vocabs_not_w2v_cnt += 1\n",
        "                        vocabs_not_w2v[word] = 1\n",
        "\n",
        "                # If you have never seen a word, append it to vocab dictionary.\n",
        "                if word not in vocabs:\n",
        "                    vocabs_cnt += 1\n",
        "                    vocabs[word] = vocabs_cnt\n",
        "                    q2n.append(vocabs_cnt)\n",
        "                else:\n",
        "                    q2n.append(vocabs[word])\n",
        "\n",
        "            # Append question as number representation\n",
        "            df.at[index, question + '_n'] = q2n\n",
        "\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "    embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "    # Build the embedding matrix\n",
        "    for word, index in vocabs.items():\n",
        "        if word in word2vec.vocab:\n",
        "            embeddings[index] = word2vec.word_vec(word)\n",
        "    del word2vec\n",
        "\n",
        "    return df, embeddings\n",
        "\n",
        "\n",
        "def split_and_zero_padding(df, max_seq_length):\n",
        "    # Split to dicts\n",
        "    X = {'left': df['자소서문장_n'], 'right': df['면접질문_n']}\n",
        "\n",
        "    # Zero padding\n",
        "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#  --\n",
        "\n",
        "class ManDist(Layer):\n",
        "    \"\"\"\n",
        "    Keras Custom Layer that calculates Manhattan Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize the layer, No need to include inputs parameter!\n",
        "    def __init__(self, **kwargs):\n",
        "        self.result = None\n",
        "        super(ManDist, self).__init__(**kwargs)\n",
        "\n",
        "    # input_shape will automatic collect input shapes to build layer\n",
        "    def build(self, input_shape):\n",
        "        super(ManDist, self).build(input_shape)\n",
        "\n",
        "    # This is where the layer's logic lives.\n",
        "    def call(self, x, **kwargs):\n",
        "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
        "        return self.result\n",
        "\n",
        "    # return output shape\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return K.int_shape(self.result)\n",
        "\n",
        "\n",
        "class EmptyWord2Vec:\n",
        "    \"\"\"\n",
        "    Just for test use.\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    word_vec = {}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO4VyrOvNYV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "2a33add8-7950-48c2-f0ce-14ee536003ec"
      },
      "source": [
        "# Load training set\n",
        "train_df = pd.read_csv('./data/dataset.csv', encoding='cp949')\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>자소서문장</th>\n",
              "      <th>면접질문</th>\n",
              "      <th>일치여부</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>전공 외에 관심 있는 학문을 말해보세요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>당사가 좀 더 글로벌한 기업이 되기 위해 어떻게 해야 하는지 의견을 말해보세요</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>컴퓨터전공을 선택한 이유는 무엇인가요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>인생에서 가장 추웠을 때는 언제인가요?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>학교에서 관심있게 들었던 전공 수업은 어떤 것인가요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66231</th>\n",
              "      <td>저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...</td>\n",
              "      <td>포트폴리오 중 자신이 가장 애착이 가는 작품은 무엇인가요?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66232</th>\n",
              "      <td>저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...</td>\n",
              "      <td>영어 어느정도 하나요? 영어로 업무해본 경험있나요?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66233</th>\n",
              "      <td>저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...</td>\n",
              "      <td>타사와 비교할 때 당사의 약점은 무엇이라고 생각하나요?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66234</th>\n",
              "      <td>저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...</td>\n",
              "      <td>살면서 가장 몰입했던 경험을 영어로 이야기 해보세요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66235</th>\n",
              "      <td>저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...</td>\n",
              "      <td>직무가 정확히 무슨일 하는지 아는가요? 혹시 선배사원 알고있습니까?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66236 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   자소서문장  ... 일치여부\n",
              "0      그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    1\n",
              "1      그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    0\n",
              "2      그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    1\n",
              "3      그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    0\n",
              "4      그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    1\n",
              "...                                                  ...  ...  ...\n",
              "66231  저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...  ...    0\n",
              "66232  저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...  ...    1\n",
              "66233  저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...  ...    0\n",
              "66234  저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...  ...    1\n",
              "66235  저의 공익 지향적 경험은 2003년도의 OO OO OOOOOO 선수촌 영어 통역 자...  ...    0\n",
              "\n",
              "[66236 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRHago2FeIB"
      },
      "source": [
        "####2-3. train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyIzPJ1stVIi"
      },
      "source": [
        "matplotlib.use('Agg')\n",
        "\n",
        "# Load training set\n",
        "train_df = pd.read_csv('./data/dataset.csv, encoding='cp949')\n",
        "for q in ['자소서문장', '면접질문']:\n",
        "    train_df[q + '_n'] = train_df[q]\n",
        "\n",
        "train_df = train_df.fillna(' ')\n",
        "\n",
        "# Make word2vec embeddings\n",
        "embedding_dim = 200\n",
        "max_seq_length = 20\n",
        "use_w2v = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "nDM_dVn5ta1k",
        "outputId": "fbffa9a6-9d02-4539-e3ac-5334db05765b"
      },
      "source": [
        "'''# 첫 실행시에만 사용\n",
        "train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n",
        "train_df.to_csv(\"./data/dataset_w2v.csv\")\n",
        "np.save('./data/embeddings', embeddings)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# 첫 실행시에만 사용\\ntrain_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\\ntrain_df.to_csv(\"./data/dataset_ver4_w2v.csv\")\\nnp.save(\\'./data/embeddings_ver4\\', embeddings)'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "PxvqHBLWtb3Y",
        "outputId": "18945ffe-0b3b-4abd-a79d-92740d613779"
      },
      "source": [
        "# 이후 실행시에는 저장된 것 불러오기\n",
        "embeddings = np.load('./data/embeddings.npy')\n",
        "train_df = pd.read_csv(\"./data/dataset_w2v.csv\", index_col=0, encoding='utf-8')\n",
        "\n",
        "train_df['자소서문장_n'] = [[int(idx) for idx in x[1:-1].split(\",\")] for x in train_df['자소서문장_n']]\n",
        "\n",
        "#train_df['면접질문_n'] = [[int(idx) for idx in x[1:-1].split(\",\")] for x in train_df['면접질문_n']]\n",
        "nidx_str = train_df.면접질문_n.apply(lambda x: x[1:-1].split(', '))\n",
        "nidx_list = []\n",
        "for idx in nidx_str:\n",
        "  if(idx[0] != ''): #리스트 안에 빈 문자열이 있는 경우\n",
        "    nidx_list.append(list(map(int, idx)))\n",
        "  else:\n",
        "    nidx_list.append(list())\n",
        "train_df['면접질문_n'] = nidx_list\n",
        "\n",
        "print(embeddings)\n",
        "train_df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 2.70576417e-01  2.03971490e-01  1.90512359e+00 ...  1.27415910e-01\n",
            "  -7.61610717e-02 -6.20568991e-01]\n",
            " [-4.84963079e-01 -9.52729472e-01 -7.64221623e-01 ... -3.30286616e-01\n",
            "  -4.92645268e-01  5.17554893e-01]\n",
            " ...\n",
            " [-8.68679311e-01 -6.02979936e-02 -5.97434768e-01 ...  9.39615033e-02\n",
            "  -1.66074377e+00 -1.06269364e+00]\n",
            " [-1.23075294e+00 -2.19805002e+00 -7.63520062e-01 ...  1.00286484e+00\n",
            "  -7.91196942e-01 -9.50767100e-01]\n",
            " [-2.04724684e-01 -2.15112482e+00 -1.66952180e-01 ... -3.19010864e-02\n",
            "   6.85219311e-04 -2.00495112e-01]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>자소서문장</th>\n",
              "      <th>면접질문</th>\n",
              "      <th>일치여부</th>\n",
              "      <th>자소서문장_n</th>\n",
              "      <th>면접질문_n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>전공 외에 관심 있는 학문을 말해보세요</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
              "      <td>[7, 4, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>당사가 좀 더 글로벌한 기업이 되기 위해 어떻게 해야 하는지 의견을 말해보세요</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
              "      <td>[12, 13, 14, 15]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>컴퓨터전공을 선택한 이유는 무엇인가요?</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
              "      <td>[5, 7, 16, 17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>인생에서 가장 추웠을 때는 언제인가요?</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
              "      <td>[8, 18, 19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...</td>\n",
              "      <td>학교에서 관심있게 들었던 전공 수업은 어떤 것인가요?</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
              "      <td>[20, 4, 7, 21]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               자소서문장  ...            면접질문_n\n",
              "0  그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...        [7, 4, 11]\n",
              "1  그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...  [12, 13, 14, 15]\n",
              "2  그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    [5, 7, 16, 17]\n",
              "3  그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...       [8, 18, 19]\n",
              "4  그렇게 어렸을 때부터 자연스럽게 IT분야에 관심을 가지던 저는 컴퓨터 공학을 전공하...  ...    [20, 4, 7, 21]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpFqryJitebO",
        "outputId": "5bdb2137-24b2-4727-aa12-ce97175495fb"
      },
      "source": [
        "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"  # 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "# 모델의 가중치를 저장하는 콜백 만들기 (다섯번째 에포크마다 가중치를 저장)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1,\n",
        "                                                 period=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UyYDB-Fgd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf4e7d4-8e2a-4290-dcff-8810071ec567"
      },
      "source": [
        "# Split to train validation\n",
        "validation_size = int(len(train_df) * 0.1)\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[['자소서문장_n', '면접질문_n']]\n",
        "Y = train_df['일치여부']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "X_train = split_and_zero_padding(X_train, max_seq_length)\n",
        "X_validation = split_and_zero_padding(X_validation, max_seq_length)\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)\n",
        "\n",
        "print(len(X_train), len(X_validation), len(Y_train), len(Y_validation))\n",
        "\n",
        "# --\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 2 59613 6623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcKZpTdtGP1l",
        "outputId": "57e36661-5ee2-4f35-c519-c81a421952a9"
      },
      "source": [
        "# --\n",
        "\n",
        "# Model variables\n",
        "gpus = 1\n",
        "batch_size = 128 * gpus   #1024 * gpus\n",
        "n_epoch = 50\n",
        "n_hidden = 40\n",
        "\n",
        "# Define the shared model\n",
        "x = Sequential()\n",
        "x.add(Embedding(len(embeddings), embedding_dim,\n",
        "                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
        "# LSTM\n",
        "x.add(LSTM(n_hidden))\n",
        "shared_model = x\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "# Pack it all up into a Manhattan Distance model\n",
        "malstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n",
        "model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
        "\n",
        "if gpus >= 2:\n",
        "    # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n",
        "    # model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    with strategy.scope():\n",
        "      model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "else:\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "shared_model.summary()\n",
        "\n",
        "# Start trainings\n",
        "training_start_time = time()\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0)) # `checkpoint_path` 포맷을 사용하는 가중치를 저장합니다\n",
        "malstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n",
        "                           batch_size=batch_size, epochs=n_epoch,\n",
        "                           validation_data=([X_validation['left'], X_validation['right']], Y_validation),\n",
        "                           callbacks=[cp_callback]) #콜백을 훈련에 전달함\n",
        "training_end_time = time()\n",
        "print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n",
        "                                                        training_end_time - training_start_time))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(211)\n",
        "plt.plot(malstm_trained.history['acc'])\n",
        "plt.plot(malstm_trained.history['val_acc'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(212)\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout(h_pad=1.0)\n",
        "plt.savefig('./data/resume-history-graph.png')\n",
        "\n",
        "print(str(malstm_trained.history['val_acc'][-1])[:6] +\n",
        "      \"(max: \" + str(max(malstm_trained.history['val_acc']))[:6] + \")\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 40)           1325960     input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "man_dist_1 (ManDist)            (None, 1)            0           sequential_1[0][0]               \n",
            "                                                                 sequential_1[1][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,325,960\n",
            "Trainable params: 38,560\n",
            "Non-trainable params: 1,287,400\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 200)           1287400   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 40)                38560     \n",
            "=================================================================\n",
            "Total params: 1,325,960\n",
            "Trainable params: 38,560\n",
            "Non-trainable params: 1,287,400\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "466/466 [==============================] - 159s 325ms/step - loss: 0.2338 - acc: 0.6752 - val_loss: 0.1800 - val_acc: 0.7457\n",
            "Epoch 2/50\n",
            "466/466 [==============================] - 150s 323ms/step - loss: 0.1681 - acc: 0.7661 - val_loss: 0.1582 - val_acc: 0.7803\n",
            "Epoch 3/50\n",
            "466/466 [==============================] - 150s 322ms/step - loss: 0.1499 - acc: 0.7965 - val_loss: 0.1446 - val_acc: 0.7996\n",
            "Epoch 4/50\n",
            "466/466 [==============================] - 150s 321ms/step - loss: 0.1378 - acc: 0.8158 - val_loss: 0.1375 - val_acc: 0.8150\n",
            "Epoch 5/50\n",
            "466/466 [==============================] - 152s 327ms/step - loss: 0.1306 - acc: 0.8278 - val_loss: 0.1331 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00005: saving model to training_ver4.2/cp-0005.ckpt\n",
            "Epoch 6/50\n",
            "466/466 [==============================] - 152s 326ms/step - loss: 0.1248 - acc: 0.8382 - val_loss: 0.1282 - val_acc: 0.8332\n",
            "Epoch 7/50\n",
            "466/466 [==============================] - 150s 323ms/step - loss: 0.1201 - acc: 0.8470 - val_loss: 0.1260 - val_acc: 0.8354\n",
            "Epoch 8/50\n",
            "466/466 [==============================] - 149s 320ms/step - loss: 0.1165 - acc: 0.8510 - val_loss: 0.1219 - val_acc: 0.8401\n",
            "Epoch 9/50\n",
            "466/466 [==============================] - 147s 317ms/step - loss: 0.1132 - acc: 0.8577 - val_loss: 0.1196 - val_acc: 0.8413\n",
            "Epoch 10/50\n",
            "466/466 [==============================] - 147s 316ms/step - loss: 0.1109 - acc: 0.8598 - val_loss: 0.1173 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00010: saving model to training_ver4.2/cp-0010.ckpt\n",
            "Epoch 11/50\n",
            "466/466 [==============================] - 148s 318ms/step - loss: 0.1092 - acc: 0.8621 - val_loss: 0.1163 - val_acc: 0.8472\n",
            "Epoch 12/50\n",
            "466/466 [==============================] - 150s 323ms/step - loss: 0.1073 - acc: 0.8658 - val_loss: 0.1153 - val_acc: 0.8492\n",
            "Epoch 13/50\n",
            "466/466 [==============================] - 147s 315ms/step - loss: 0.1059 - acc: 0.8682 - val_loss: 0.1145 - val_acc: 0.8514\n",
            "Epoch 14/50\n",
            "466/466 [==============================] - 147s 315ms/step - loss: 0.1044 - acc: 0.8701 - val_loss: 0.1145 - val_acc: 0.8525\n",
            "Epoch 15/50\n",
            "466/466 [==============================] - 147s 315ms/step - loss: 0.1036 - acc: 0.8725 - val_loss: 0.1134 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00015: saving model to training_ver4.2/cp-0015.ckpt\n",
            "Epoch 16/50\n",
            "466/466 [==============================] - 148s 318ms/step - loss: 0.1026 - acc: 0.8747 - val_loss: 0.1130 - val_acc: 0.8529\n",
            "Epoch 17/50\n",
            "466/466 [==============================] - 150s 323ms/step - loss: 0.1019 - acc: 0.8744 - val_loss: 0.1122 - val_acc: 0.8560\n",
            "Epoch 18/50\n",
            "466/466 [==============================] - 149s 320ms/step - loss: 0.1010 - acc: 0.8765 - val_loss: 0.1119 - val_acc: 0.8551\n",
            "Epoch 19/50\n",
            "466/466 [==============================] - 150s 323ms/step - loss: 0.1003 - acc: 0.8788 - val_loss: 0.1121 - val_acc: 0.8551\n",
            "Epoch 20/50\n",
            "466/466 [==============================] - 153s 328ms/step - loss: 0.0996 - acc: 0.8785 - val_loss: 0.1125 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00020: saving model to training_ver4.2/cp-0020.ckpt\n",
            "Epoch 21/50\n",
            "466/466 [==============================] - 161s 345ms/step - loss: 0.0991 - acc: 0.8804 - val_loss: 0.1112 - val_acc: 0.8552\n",
            "Epoch 22/50\n",
            "466/466 [==============================] - 162s 347ms/step - loss: 0.0983 - acc: 0.8805 - val_loss: 0.1109 - val_acc: 0.8555\n",
            "Epoch 23/50\n",
            "466/466 [==============================] - 160s 343ms/step - loss: 0.0978 - acc: 0.8819 - val_loss: 0.1104 - val_acc: 0.8558\n",
            "Epoch 24/50\n",
            "466/466 [==============================] - 160s 343ms/step - loss: 0.0975 - acc: 0.8820 - val_loss: 0.1108 - val_acc: 0.8573\n",
            "Epoch 25/50\n",
            "466/466 [==============================] - 161s 345ms/step - loss: 0.0966 - acc: 0.8838 - val_loss: 0.1102 - val_acc: 0.8570\n",
            "\n",
            "Epoch 00025: saving model to training_ver4.2/cp-0025.ckpt\n",
            "Epoch 26/50\n",
            "466/466 [==============================] - 162s 347ms/step - loss: 0.0963 - acc: 0.8839 - val_loss: 0.1107 - val_acc: 0.8575\n",
            "Epoch 27/50\n",
            "466/466 [==============================] - 157s 337ms/step - loss: 0.0959 - acc: 0.8848 - val_loss: 0.1096 - val_acc: 0.8572\n",
            "Epoch 28/50\n",
            "466/466 [==============================] - 150s 321ms/step - loss: 0.0953 - acc: 0.8854 - val_loss: 0.1091 - val_acc: 0.8576\n",
            "Epoch 29/50\n",
            "466/466 [==============================] - 150s 322ms/step - loss: 0.0948 - acc: 0.8871 - val_loss: 0.1091 - val_acc: 0.8587\n",
            "Epoch 30/50\n",
            "466/466 [==============================] - 150s 322ms/step - loss: 0.0946 - acc: 0.8875 - val_loss: 0.1098 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00030: saving model to training_ver4.2/cp-0030.ckpt\n",
            "Epoch 31/50\n",
            "466/466 [==============================] - 150s 322ms/step - loss: 0.0943 - acc: 0.8868 - val_loss: 0.1093 - val_acc: 0.8570\n",
            "Epoch 32/50\n",
            "466/466 [==============================] - 147s 316ms/step - loss: 0.0941 - acc: 0.8874 - val_loss: 0.1078 - val_acc: 0.8617\n",
            "Epoch 33/50\n",
            "466/466 [==============================] - 146s 313ms/step - loss: 0.0935 - acc: 0.8881 - val_loss: 0.1084 - val_acc: 0.8646\n",
            "Epoch 34/50\n",
            "466/466 [==============================] - 149s 319ms/step - loss: 0.0932 - acc: 0.8899 - val_loss: 0.1080 - val_acc: 0.8606\n",
            "Epoch 35/50\n",
            "466/466 [==============================] - 148s 317ms/step - loss: 0.0931 - acc: 0.8885 - val_loss: 0.1081 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00035: saving model to training_ver4.2/cp-0035.ckpt\n",
            "Epoch 36/50\n",
            "466/466 [==============================] - 148s 318ms/step - loss: 0.0929 - acc: 0.8899 - val_loss: 0.1083 - val_acc: 0.8614\n",
            "Epoch 37/50\n",
            "466/466 [==============================] - 146s 313ms/step - loss: 0.0923 - acc: 0.8897 - val_loss: 0.1064 - val_acc: 0.8624\n",
            "Epoch 38/50\n",
            "466/466 [==============================] - 147s 315ms/step - loss: 0.0920 - acc: 0.8918 - val_loss: 0.1078 - val_acc: 0.8596\n",
            "Epoch 39/50\n",
            "466/466 [==============================] - 146s 314ms/step - loss: 0.0918 - acc: 0.8914 - val_loss: 0.1069 - val_acc: 0.8620\n",
            "Epoch 40/50\n",
            "466/466 [==============================] - 146s 313ms/step - loss: 0.0914 - acc: 0.8914 - val_loss: 0.1068 - val_acc: 0.8634\n",
            "\n",
            "Epoch 00040: saving model to training_ver4.2/cp-0040.ckpt\n",
            "Epoch 41/50\n",
            "466/466 [==============================] - 145s 312ms/step - loss: 0.0909 - acc: 0.8924 - val_loss: 0.1064 - val_acc: 0.8629\n",
            "Epoch 42/50\n",
            "466/466 [==============================] - 146s 312ms/step - loss: 0.0908 - acc: 0.8931 - val_loss: 0.1065 - val_acc: 0.8674\n",
            "Epoch 43/50\n",
            "466/466 [==============================] - 146s 313ms/step - loss: 0.0905 - acc: 0.8938 - val_loss: 0.1063 - val_acc: 0.8628\n",
            "Epoch 44/50\n",
            "466/466 [==============================] - 146s 312ms/step - loss: 0.0903 - acc: 0.8933 - val_loss: 0.1069 - val_acc: 0.8659\n",
            "Epoch 45/50\n",
            "466/466 [==============================] - 148s 317ms/step - loss: 0.0901 - acc: 0.8940 - val_loss: 0.1053 - val_acc: 0.8655\n",
            "\n",
            "Epoch 00045: saving model to training_ver4.2/cp-0045.ckpt\n",
            "Epoch 46/50\n",
            "466/466 [==============================] - 147s 315ms/step - loss: 0.0899 - acc: 0.8942 - val_loss: 0.1066 - val_acc: 0.8640\n",
            "Epoch 47/50\n",
            "466/466 [==============================] - 149s 319ms/step - loss: 0.0897 - acc: 0.8945 - val_loss: 0.1058 - val_acc: 0.8656\n",
            "Epoch 48/50\n",
            "466/466 [==============================] - 148s 317ms/step - loss: 0.0895 - acc: 0.8950 - val_loss: 0.1065 - val_acc: 0.8689\n",
            "Epoch 49/50\n",
            "466/466 [==============================] - 147s 316ms/step - loss: 0.0892 - acc: 0.8958 - val_loss: 0.1055 - val_acc: 0.8667\n",
            "Epoch 50/50\n",
            "466/466 [==============================] - 147s 316ms/step - loss: 0.0891 - acc: 0.8955 - val_loss: 0.1056 - val_acc: 0.8677\n",
            "\n",
            "Epoch 00050: saving model to training_ver4.2/cp-0050.ckpt\n",
            "Training time finished.\n",
            "50 epochs in      7509.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8677(max: 0.8689)\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}